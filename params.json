{"name":"Digit-recognition-using-neural-networks","tagline":"Recognize handwritten digits using neural networks.","body":"# Neural-Network to recognize digits\r\n###Training a net to recognize handwritten digits using MNIST training date\r\nVersion 1 ( 31st August 2015) [ training done on a batch of 100 with 500 epochs]\r\n\r\n## This code is free to use. If you are using this code kindly cite my [github](https://github.com/khannasarthak) and [linkedin](https://www.linkedin.com/in/sarthakkhanna) profiles in the references. \r\n\r\n\r\n###Shortcomings in this version:\r\n\r\n- 7 it always displays 2\r\n- 9 it displays 2,3\r\n- 1 if displays 1 only if the line drawn is in the center, other wise it often shows 8 or 3\r\n- 4 if displays correctly only if 4 is written towards the upper right of the 28x28 matrix\r\n- 8,5,2,0 are shown correct if legible\r\n\r\n THe results are better if we increase the batchSIze and the number of epochs, which resulted in only the number 7 not being recognised correctly.\r\n\r\n  ###Functions explained:\r\n\r\n  ####main.m\r\n\r\n  The main function, does the following:\r\n  - loads the MNIST training images and labels ( using helper functions)\r\n  - initialises the number of hidden Units, batch size and learning rate alpha\r\n  - calls the function to train the net [trainNet]\r\n  - writes the hidden and output variables to a text file as CSV's\r\n  - loads the MNIST validation images and labels\r\n  - compares the output from our training set and the validation set\r\n  - tells the number of correctly classified images and the errors\r\n\r\n  ####sigmoid.m\r\n\r\n  Returns the logistic sigmoid of the input, done element by element\r\n\r\n  ####dsigmoid.m\r\n\r\n  Returns the derivative of logistic sigmoid to use in Back Propagation\r\n\r\n  ####trainNet.m\r\n\r\n  Trains our neural net using Back Propogation and also using Batch training.\r\n\r\n  In batch training the weights are updated batchSize times where random input vectors are chosen and the weights are decided.\r\n  Epochs are the number of times the above procedure is repeated, generally, the more the number of epochs the lower the error rate.\r\n\r\n  Function does the following:\r\n  - implement batchwise backpropogation and generates weights\r\n  - plots the Mean Square Error rate with respect to number of epochs\r\n\r\n  ####validation.m\r\n\r\n  It validates on the MNIST validation set. Does the folloing:\r\n  - using the weights learned in trainNet.m, it predicts the validation set\r\n  - gives the output as the class ( i.e the number that is written)\r\n\r\n  ####readNew.m\r\n\r\n  Takes in a 28x28 pixel grayscale image and classifies it as which number it is. Does the following:\r\n  - reads image\r\n  - read hidden weights written in CSV's earlier in trainNet.m\r\n  - convert the image to a 784x1 input vector\r\n  - using hidden and output weights, feeds forward the input to get the final output neuron values\r\n  - gives the value that the net has recognised\r\n\r\n\r\n\r\n References : ([http://davidstutz.de/recognizing-handwritten-digits-mnist-dataset-twolayer-perceptron/](http://davidstutz.de/recognizing-handwritten-digits-mnist-dataset-twolayer-perceptron/))\r\n","google":"UA-67024998-1","note":"Don't delete this file! It's used internally to help with page regeneration."}